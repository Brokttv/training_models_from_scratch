# training_models_from_scratch

# Training LLMs from Scratch â€” Math + NumPy Only

Today, I'll be training LLMs from absolute scratch using both **math** and **code**, with **no deep learning libraries** â€” just **NumPy**.  
And **you** should learn to do that too. Here's why:

## Why Do This?

- ğŸ§  Build an **intuitive understanding** of whatâ€™s going on under the hood.  
- âš™ï¸ The low-level logic is beautiful â€” once you get this, everything else (PyTorch, TensorFlow, Transformers) becomes **butter-smooth**.
- ğŸ” Learning NumPy first makes transitioning to PyTorch seamless â€” they share similar operations and logic.
- ğŸ› ï¸ Youâ€™ll become a **debugging master** â€” because you'll actually understand what each layer and step is doing.



## ğŸš€ Letâ€™s Get Started

Everything is built with:
- ğŸ **Python**
- ğŸ§® **NumPy**
- âŒ No high-level ML libraries

Ready to get your hands dirty with raw math and code?

---

## ğŸ—‚ï¸ Agenda

### ğŸ”¹ Warm-up: Linear & Logistic Regression

We'll start simple:
- Train a **one-input linear regression** model
- Build a **single-neuron logistic regression** model

> âš ï¸ **Stuck along the way?**  
> This [YouTube playlist](https://youtube.com/playlist?list=PLeM4O8deP8GO3vIx_9eboO9tVpUKHYqRg&si=_qPsNeX3TuMZS9sf) by Professor Bryce is a treasure â€” highly recommended!


![LLM Training](â€œimageâ€)


### ğŸ”¸ Next: Neural Networks from Scratch

Weâ€™ll train:
- A **single-input, 2-layer fully connected neural network**  
- A **multi-input version** using **ReLU activation**  
- All for **regression tasks**

During this part, weâ€™ll dive into:
- The importance of **non-linearities**
- How simple mathematical tools build up to modern deep learning

---


